{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generate Test Data\n",
    "\n",
    "Notebook to generate the data required for testing technical knowledge.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai import OpenAI, OpenAIError\n",
    "import json\n",
    "from pathlib import Path\n",
    "import glob\n",
    "import uuid\n",
    "import sys"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configurations\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "SYSTEM_CONTEXT = \"\"\"\n",
    "You are a multiple-choice test that is assessing knowledge on the following topics:\n",
    "- Computer Science\n",
    "- Data Science\n",
    "- Data Engineering\n",
    "- Web Development\n",
    "- AWS\n",
    "- AI\n",
    "- ML\n",
    "- DevOps\n",
    "- Platform Engineering\n",
    "- React\n",
    "- JavaScript\n",
    "- Python\n",
    "- Cloud Platforms\n",
    "\n",
    "The test starts by assessing the breadth of knowledge. After these questions are completed, the user has the option to show the results or to continue on to the depth assessment. \n",
    "\n",
    "When assessing the breadth of knowledge ask 5 questions for each topic. Each question should get progressively more difficult. This should make the total question count for the bread assessment 5 * the number of topics. To complete thee breadth assessment all topics must completed. Don't offer to show the results until all topics have been completed.\n",
    "\n",
    "When assessing the depth of knowledge ask 50 questions for each topic. Each question should get progressively more difficult based on the accuracy of the previous response. If there are 5 wrong answers in a row, advance to the next topic. A user can choose to select a subset of the topics for the depth assessment.\n",
    "\n",
    "After all topics have been assessed, a report is shown with the results. The report should include information about both the breadth and depth of knowledge on each topic. The report should include a scale showing the level of knowledge for each topic. The scale should be from 0 - 100. The report should conclude with recommendations for other related topics to test.\n",
    "\n",
    "The difficulty level of questions should range from 1 - 5, with 1 being the easiest and 5 being the hardest.\n",
    "\n",
    "To answer a question, a user specifies the letter of the answer they want to select. The user should say \"?\" if they do not know the answer.\n",
    "\"\"\".strip()\n",
    "\n",
    "TOPICS = [\n",
    "    \"Artificial Intelligence\",\n",
    "    \"Automated Testing\",\n",
    "    \"AWS\",\n",
    "    \"Cloud Platforms\",\n",
    "    \"Cloud Security\",\n",
    "    \"Computer Science\",\n",
    "    \"Continuous Delivery\",\n",
    "    \"Continuous Deployment\",\n",
    "    \"Continuous Integration\",\n",
    "    \"Cybersecurity\",\n",
    "    \"Data Analysis\",\n",
    "    \"Data Engineering\",\n",
    "    \"Data Science\",\n",
    "    \"DevOps\",\n",
    "    \"Enterprise Architecture\",\n",
    "    \"Git\",\n",
    "    \"Infrastructure as Code\",\n",
    "    \"JavaScript\",\n",
    "    \"Machine Learning\",\n",
    "    \"Microservices\",\n",
    "    \"Network Security\",\n",
    "    \"Penetration Testing\",\n",
    "    \"Platform Engineering\",\n",
    "    \"Python\",\n",
    "    \"React\",\n",
    "    \"Site Reliability Engineering\",\n",
    "    \"Software Architecture\",\n",
    "    \"Software Engineering\",\n",
    "    \"Solutions Architecture\",\n",
    "    \"Web Development\",\n",
    "]\n",
    "\n",
    "# Uncomment features to enable them\n",
    "ENABLED_FEATURES = [\n",
    "    # \"QUESTION_GENERATION\",\n",
    "    # \"CHOICE_GENERATION\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "client = OpenAI()\n",
    "default_messages = [{\"role\": \"system\", \"content\": SYSTEM_CONTEXT}]\n",
    "default_model = \"gpt-4-turbo-preview\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Utilities\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ask(question, messages=default_messages, model=default_model):\n",
    "    try:\n",
    "        conversation = []\n",
    "        conversation.extend(messages)\n",
    "        conversation.extend([{\"role\": \"user\", \"content\": question}])\n",
    "        response = client.chat.completions.create(\n",
    "            model=model,\n",
    "            messages=conversation,\n",
    "            response_format={\"type\": \"json_object\"},\n",
    "        )\n",
    "        complete_message = response.choices[0].message.content\n",
    "        conversation.extend([{\"role\": \"assistant\", \"content\": complete_message}])\n",
    "\n",
    "        return conversation\n",
    "    except OpenAIError as e:\n",
    "        # Handle all OpenAI API errors\n",
    "        print(f\"Error: {e}\")\n",
    "        sys.exit()\n",
    "\n",
    "\n",
    "def generate_questions(topics):\n",
    "    for current_topic in topics:\n",
    "        current_key = \"_\".join(word.lower() for word in current_topic.split())\n",
    "        data_path = f\"./data/{current_key}_data.json\"\n",
    "\n",
    "        if not Path(data_path).exists():\n",
    "            current_messages = ask(\n",
    "                f\"Provide a comprehensive list of questions for the {current_topic} topic. Provide 100 questions total, with 20 questions for each difficulty level. Sort the list by difficulty level from easiest to hardest. The response should be a list of strings. The list only contains questions. Each question should be format as '[difficulty]: [question]'. Eg. '1: What does AI stand for?' Do not include the choices. Do not include the question number. Return the list formatted as JSON.\"\n",
    "            )\n",
    "            Path(data_path).write_text(current_messages[-1][\"content\"])\n",
    "\n",
    "\n",
    "def generate_choices(topics):\n",
    "    for current_topic in topics:\n",
    "        current_key = \"_\".join(word.lower() for word in current_topic.split())\n",
    "        data_path = f\"./data/{current_key}_data.json\"\n",
    "        data = json.loads(Path(data_path).read_text())\n",
    "        questions = data[\"questions\"]\n",
    "        data[\"choices\"] = [] if \"choices\" not in data else data[\"choices\"]\n",
    "        existing_choices = [choice[\"question_id\"] for choice in data[\"choices\"]]\n",
    "\n",
    "        for question in questions:\n",
    "            if question[\"id\"] not in existing_choices:\n",
    "                print(f\"Generating choices ({current_topic}): {question['question']}\")\n",
    "                prompt_message = f\"I'm creating a multiple choice test on a specific topic. Generate 4 choices for the provided question. There should be only one correct choice. Randomize the placement of the correct choice.\"\n",
    "                prompt_details = (\n",
    "                    f\"Topic: {current_topic}\\n\\nQuestion: {question['question']}\"\n",
    "                )\n",
    "                prompt_format_instructions = 'Return the response as JSON. Example: { \"choices\": [\"first example choice\", \"second example choice\", ...otherChoices], \"answer\": <index of correct choice> }'\n",
    "                prompt = f\"{prompt_message}\\n\\n{prompt_details}\\n\\n{prompt_format_instructions}\"\n",
    "                current_messages = ask(prompt)\n",
    "                question_choices = json.loads(current_messages[-1][\"content\"])\n",
    "                question_choices[\"question_id\"] = question[\"id\"]\n",
    "                data[\"choices\"].append(question_choices)\n",
    "                Path(data_path).write_text(json.dumps(data, indent=2))\n",
    "\n",
    "\n",
    "def transform_questions(topics):\n",
    "    for current_topic in topics:\n",
    "        transformed_questions = []\n",
    "        current_key = \"_\".join(word.lower() for word in current_topic.split())\n",
    "        data_path = f\"./data/{current_key}_data.json\"\n",
    "        data = json.loads(Path(data_path).read_text())\n",
    "        questions = data[\"questions\"]\n",
    "\n",
    "        for question_raw in questions:\n",
    "            if isinstance(question_raw, str):\n",
    "                difficulty = question_raw[0:1]\n",
    "                question_text = question_raw[3:]\n",
    "                question_dict = {\n",
    "                    \"id\": str(uuid.uuid4()),\n",
    "                    \"difficulty\": int(difficulty),\n",
    "                    \"question\": question_text,\n",
    "                }\n",
    "            elif isinstance(question_raw, dict):\n",
    "                question_dict = question_raw\n",
    "                question_dict.setdefault(\"id\", str(uuid.uuid4()))\n",
    "                question_dict.setdefault(\"difficulty\", None)\n",
    "                question_dict.setdefault(\"question\", None)\n",
    "\n",
    "            transformed_questions.append(question_dict)\n",
    "\n",
    "        Path(data_path).write_text(\n",
    "            json.dumps(\n",
    "                {\"topic\": current_topic, \"questions\": transformed_questions}, indent=2\n",
    "            )\n",
    "        )\n",
    "\n",
    "\n",
    "def read_data(glob_pattern):\n",
    "    files_contents = {}\n",
    "    for file_path in glob.glob(glob_pattern):\n",
    "        try:\n",
    "            with open(file_path, \"r\") as file:\n",
    "                files_contents[file_path] = file.read()\n",
    "\n",
    "            if file_path.endswith(\".json\"):\n",
    "                files_contents[file_path] = json.loads(files_contents[file_path])\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Error reading {file_path}: {e}\")\n",
    "\n",
    "    return files_contents\n",
    "\n",
    "\n",
    "def is_enabled(feature_name):\n",
    "    if feature_name in ENABLED_FEATURES:\n",
    "        return True\n",
    "    else:\n",
    "        print(f\"feature disabled: {feature_name}\")\n",
    "        return False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate Questions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "feature disabled: QUESTION_GENERATION\n"
     ]
    }
   ],
   "source": [
    "if is_enabled(\"QUESTION_GENERATION\"):\n",
    "    generate_questions(TOPICS)\n",
    "    transform_questions(TOPICS)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate Choices\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "feature disabled: CHOICE_GENERATION\n"
     ]
    }
   ],
   "source": [
    "if is_enabled(\"CHOICE_GENERATION\"):\n",
    "    generate_choices(TOPICS)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = read_data(\"./data/*.json\")\n",
    "\n",
    "for file_path, datum in data.items():\n",
    "    print(f\"Loaded file: {file_path}\\n{datum}\\n\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py311",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
